final_regressor = model
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = ../../data/train.vw.cache
Reading datafile = ../../data/train.vw
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.624359   0.624359            3         3.0   1.0000   0.1981        9
0.631322   0.638286            6         6.0  -1.0000   0.4810        9
0.520916   0.388429           11        11.0   1.0000   1.2335        9
0.400726   0.280537           22        22.0   1.0000   1.9233        9
0.345550   0.290373           44        44.0   1.0000   1.8549        9
0.304542   0.262581           87        87.0   1.0000   2.8049        9
0.301186   0.297829          174       174.0   1.0000   3.0535        9
0.286284   0.271382          348       348.0   1.0000   2.6956        9
0.249120   0.211955          696       696.0   1.0000   3.8945        9
0.239662   0.230204         1392      1392.0  -1.0000   3.0468        9
0.225099   0.210537         2784      2784.0   1.0000   3.7897        9
0.206387   0.187675         5568      5568.0  -1.0000   3.3753        9
0.198000   0.189612        11135     11135.0  -1.0000   2.0410        9
0.187934   0.177866        22269     22269.0   1.0000   2.6882        9
0.167171   0.146407        44537     44537.0   1.0000   4.6527        9
0.146607   0.126043        89073     89073.0   1.0000   2.6663        9
0.125771   0.104934       178146    178146.0   1.0000   5.0509        9

finished run
number of examples = 327690
weighted example sum = 327690
weighted label sum = 289750
average loss = 0.109394
best constant = 0.88422
total feature number = 2949210
only testing
Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
predictions = p.txt
using no cache
Reading datafile = ../../data/test.vw
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
12.328730  12.328730           3         3.0   3.0000   7.2713        9
8.860565   5.392400            6         6.0   6.0000   6.6205        9
18.092636  29.171121          11        11.0  11.0000   4.1981        9
99.761213  181.429791         22        22.0  22.0000   5.3936        9
488.899350 878.037487         44        44.0  44.0000   6.1628        9
2123.957510 3797.040278         87        87.0  87.0000   3.4178        9
9354.674999 16585.392488        174       174.0 174.0000   6.2466        9
38926.290018 68497.905038        348       348.0 348.0000   3.2066        9
158681.640081 278436.990145        696       696.0 696.0000   1.5058        9
639996.611303 1121311.582525       1392      1392.0 1392.0000   3.6556        9
2572013.981747 4504031.352191       2784      2784.0 2784.0000   2.0428        9
10311150.710522 18050287.439296       5568      5568.0 5568.0000   1.7758        9
41283051.506079 72260515.783007      11135     11135.0 11135.0000   3.5707        9
165210943.265714 289149965.606610      22269     22269.0 22269.0000   8.3541        9
660996803.826036 1156804928.885216      44537     44537.0 44537.0000   3.0823        9

finished run
number of examples = 58921
weighted example sum = 58921
weighted label sum = 1.73587e+09
average loss = 1.15698e+09
best constant = 29461.5
total feature number = 530289
