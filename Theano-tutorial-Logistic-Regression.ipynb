{
 "metadata": {
  "name": "Theano-tutorial-Logistic-Regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Part 3\n",
      "#source: http://deeplearning.net/software/theano/tutorial/examples.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#imports\n",
      "import numpy as np\n",
      "from theano import function, shared\n",
      "import theano.tensor as T\n",
      "\n",
      "#Data and some parameters\n",
      "rng = np.random\n",
      "N = 4000\n",
      "feats = 20\n",
      "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
      "training_steps = 10000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "whos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Variable         Type              Data/Info\n",
        "--------------------------------------------\n",
        "D                tuple             n=2\n",
        "N                int               400\n",
        "T                module            <module 'theano.tensor' f<...>ano/tensor/__init__.pyc'>\n",
        "feats            int               20\n",
        "function         function          <function function at 0x107dc8f50>\n",
        "rng              module            <module 'numpy.random' fr<...>mpy/random/__init__.pyc'>\n",
        "shared           function          <function shared at 0x107d5e8c0>\n",
        "training_steps   int               10000\n",
        "x                TensorVariable    x\n",
        "y                TensorVariable    y\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Declare Theano symbolic variables\n",
      "x = T.matrix(\"x\")#feature matrix\n",
      "y = T.vector(\"y\")#labels\n",
      "w = shared(rng.randn(feats), name=\"w\")#shared symbol: linear weights which are randomly initialized\n",
      "b = shared(0., name=\"b\")#shared symbol: intercept\n",
      "print \"Initial model:\"\n",
      "print w.get_value(), b.get_value()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initial model:\n",
        "[ 0.32190275 -0.746525    0.97647971  1.25143934  0.96461383 -1.48231393\n",
        "  0.2227637  -2.17410638  0.42361817 -0.23839952 -0.7340064  -0.45718621\n",
        " -0.92111528  0.12109266  0.38324932 -1.06461356  0.15481248 -1.3304314\n",
        "  0.32728083  1.17153439] 0.0\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct Theano expression graph\n",
      "\n",
      "#we will do a l2 regularized logistic regression here\n",
      "#the training loss function will be the E_{data}[- log likelihood]\n",
      "\n",
      "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
      "prediction = p_1 > 0.5                    # The prediction thresholded\n",
      "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
      "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
      "\n",
      "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
      "                                          # (we shall return to this in a\n",
      "                                          # following section of this tutorial)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compile\n",
      "train = function(\n",
      "          inputs=[x,y],\n",
      "          outputs=[prediction, xent],\n",
      "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
      "predict = function(inputs=[x], outputs=prediction)\n",
      "\n",
      "# Train\n",
      "for i in range(training_steps):\n",
      "    pred, err = train(D[0], D[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Final model:\"\n",
      "print w.get_value(), b.get_value()\n",
      "print \"(predicted,target) values for D:\", zip(predict(D[0]),D[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-13-ade5809afcba>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-ade5809afcba>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print \"in sample error:\", sum([x[0]==x[1] for x in zip(predict(D[0]),D[1])]\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"in sample error:\", 1.0*sum([z1==z2 for (z1,z2) in zip(predict(D[0]),D[1])])/N"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in sample error: 0.58\n"
       ]
      }
     ],
     "prompt_number": 17
    }
   ],
   "metadata": {}
  }
 ]
}