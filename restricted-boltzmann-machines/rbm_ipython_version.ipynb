{
 "metadata": {
  "name": "",
  "signature": "sha256:f51c27dfd9d6a0bfabac9d6173f84b6683ec4bc877815126162d7fe0a0b6d51e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#https://github.com/echen/restricted-boltzmann-machines\n",
      "#http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/\n",
      "\n",
      "#Additional comments by Theja 2014-05-19\n",
      "\n",
      "from __future__ import print_function\n",
      "import numpy as np\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RBM:\n",
      "  \n",
      "  def __init__(self, num_visible=6, num_hidden=2, learning_rate = 0.1):\n",
      "    self.num_hidden    = num_hidden #number of hidden units\n",
      "    self.num_visible   = num_visible #number of visible units: size of a single training obs\n",
      "    self.learning_rate = learning_rate #learning rate is the constant in the weight update\n",
      "\n",
      "    # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using\n",
      "    # a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
      "    np.random.seed(0) #for predictability\n",
      "    self.weights = 0.1 * np.random.randn(self.num_visible, self.num_hidden)    \n",
      "    # Insert weights for the bias units into the first row and first column.\n",
      "    self.weights = np.insert(self.weights, 0, 0, axis = 0) #bias value of 0 inserted for all columns as the top row\n",
      "    self.weights = np.insert(self.weights, 0, 0, axis = 1) #bias value of 0 inserted for all rows as the first column\n",
      "\n",
      "  def train(self, data, max_epochs = 5000):\n",
      "    \"\"\"\n",
      "    Train the machine.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data: A matrix where each row is a training example consisting of the states of visible units.    \n",
      "    \"\"\"\n",
      "\n",
      "    num_examples = data.shape[0]\n",
      "\n",
      "    # Insert bias units of 1 into the first column.\n",
      "    data = np.insert(data, 0, 1, axis = 1)\n",
      "    \n",
      "    #     >>> a = np.array([[1, 1], [2, 2], [3, 3]])\n",
      "    #     >>> a\n",
      "    #     array([[1, 1],\n",
      "    #            [2, 2],\n",
      "    #            [3, 3]])\n",
      "    #     >>> np.insert(a, 1, 5)\n",
      "    #     array([1, 5, 1, 2, 2, 3, 3])\n",
      "    #     >>> np.insert(a, 1, 5, axis=1)\n",
      "    #     array([[1, 5, 1],\n",
      "    #            [2, 5, 2],\n",
      "    #            [3, 5, 3]])\n",
      "    \n",
      "\n",
      "    for epoch in range(max_epochs):      \n",
      "      # Clamp to the data and sample from the hidden units. \n",
      "      # (This is the \"positive CD phase\", aka the reality phase.)\n",
      "      pos_hidden_activations = np.dot(data, self.weights)      \n",
      "      pos_hidden_probs = self._logistic(pos_hidden_activations)\n",
      "      pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
      "      # Note that we're using the activation *probabilities* of the hidden states, not the hidden states       \n",
      "      # themselves, when computing associations. We could also use the states; see section 3 of Hinton's \n",
      "      # \"A Practical Guide to Training Restricted Boltzmann Machines\" for more.\n",
      "      pos_associations = np.dot(data.T, pos_hidden_probs)\n",
      "\n",
      "      # Reconstruct the visible units and sample again from the hidden units.\n",
      "      # (This is the \"negative CD phase\", aka the daydreaming phase.)\n",
      "      neg_visible_activations = np.dot(pos_hidden_states, self.weights.T)\n",
      "      neg_visible_probs = self._logistic(neg_visible_activations)\n",
      "      neg_visible_probs[:,0] = 1 # Fix the bias unit.\n",
      "      neg_hidden_activations = np.dot(neg_visible_probs, self.weights)\n",
      "      neg_hidden_probs = self._logistic(neg_hidden_activations)\n",
      "      # Note, again, that we're using the activation *probabilities* when computing associations, not the states \n",
      "      # themselves.\n",
      "      neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
      "\n",
      "      # Update weights.\n",
      "      self.weights += self.learning_rate * ((pos_associations - neg_associations) / num_examples)\n",
      "\n",
      "      error = np.sum((data - neg_visible_probs) ** 2)\n",
      "      #print(\"Epoch %s: error is %s\" % (epoch, error))\n",
      "\n",
      "  def run_visible(self, data):\n",
      "    \"\"\"\n",
      "    Assuming the RBM has been trained (so that weights for the network have been learned),\n",
      "    run the network on a set of visible units, to get a sample of the hidden units.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data: A matrix where each row consists of the states of the visible units.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    hidden_states: A matrix where each row consists of the hidden units activated from the visible\n",
      "    units in the data matrix passed in.\n",
      "    \"\"\"\n",
      "    \n",
      "    num_examples = data.shape[0]\n",
      "    \n",
      "    # Create a matrix, where each row is to be the hidden units (plus a bias unit)\n",
      "    # sampled from a training example.\n",
      "    hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
      "    \n",
      "    # Insert bias units of 1 into the first column of data.\n",
      "    data = np.insert(data, 0, 1, axis = 1)\n",
      "\n",
      "    # Calculate the activations of the hidden units.\n",
      "    hidden_activations = np.dot(data, self.weights)\n",
      "    # Calculate the probabilities of turning the hidden units on.\n",
      "    hidden_probs = self._logistic(hidden_activations)\n",
      "    # Turn the hidden units on with their specified probabilities.\n",
      "    hidden_states[:,:] = hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
      "    # Always fix the bias unit to 1.\n",
      "    # hidden_states[:,0] = 1\n",
      "  \n",
      "    # Ignore the bias units.\n",
      "    hidden_states = hidden_states[:,1:]\n",
      "    return hidden_states\n",
      "    \n",
      "  # TODO: Remove the code duplication between this method and `run_visible`?\n",
      "  def run_hidden(self, data):\n",
      "    \"\"\"\n",
      "    Assuming the RBM has been trained (so that weights for the network have been learned),\n",
      "    run the network on a set of hidden units, to get a sample of the visible units.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data: A matrix where each row consists of the states of the hidden units.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    visible_states: A matrix where each row consists of the visible units activated from the hidden\n",
      "    units in the data matrix passed in.\n",
      "    \"\"\"\n",
      "\n",
      "    num_examples = data.shape[0]\n",
      "\n",
      "    # Create a matrix, where each row is to be the visible units (plus a bias unit)\n",
      "    # sampled from a training example.\n",
      "    visible_states = np.ones((num_examples, self.num_visible + 1))\n",
      "\n",
      "    # Insert bias units of 1 into the first column of data.\n",
      "    data = np.insert(data, 0, 1, axis = 1)\n",
      "\n",
      "    # Calculate the activations of the visible units.\n",
      "    visible_activations = np.dot(data, self.weights.T)\n",
      "    # Calculate the probabilities of turning the visible units on.\n",
      "    visible_probs = self._logistic(visible_activations)\n",
      "    # Turn the visible units on with their specified probabilities.\n",
      "    visible_states[:,:] = visible_probs > np.random.rand(num_examples, self.num_visible + 1)\n",
      "    # Always fix the bias unit to 1.\n",
      "    # visible_states[:,0] = 1\n",
      "\n",
      "    # Ignore the bias units.\n",
      "    visible_states = visible_states[:,1:]\n",
      "    return visible_states\n",
      "    \n",
      "  def daydream(self, num_samples):\n",
      "    \"\"\"\n",
      "    Randomly initialize the visible units once, and start running alternating Gibbs sampling steps\n",
      "    (where each step consists of updating all the hidden units, and then updating all of the visible units),\n",
      "    taking a sample of the visible units at each step.\n",
      "    Note that we only initialize the network *once*, so these samples are correlated.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    samples: A matrix, where each row is a sample of the visible units produced while the network was\n",
      "    daydreaming.\n",
      "    \"\"\"\n",
      "\n",
      "    # Create a matrix, where each row is to be a sample of of the visible units \n",
      "    # (with an extra bias unit), initialized to all ones.\n",
      "    samples = np.ones((num_samples, self.num_visible + 1))\n",
      "\n",
      "    # Take the first sample from a uniform distribution.\n",
      "    samples[0,1:] = np.random.rand(self.num_visible)\n",
      "\n",
      "    # Start the alternating Gibbs sampling.\n",
      "    # Note that we keep the hidden units binary states, but leave the\n",
      "    # visible units as real probabilities. See section 3 of Hinton's\n",
      "    # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
      "    # for more on why.\n",
      "    for i in range(1, num_samples):\n",
      "      visible = samples[i-1,:]\n",
      "\n",
      "      # Calculate the activations of the hidden units.\n",
      "      hidden_activations = np.dot(visible, self.weights)      \n",
      "      # Calculate the probabilities of turning the hidden units on.\n",
      "      hidden_probs = self._logistic(hidden_activations)\n",
      "      # Turn the hidden units on with their specified probabilities.\n",
      "      hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
      "      # Always fix the bias unit to 1.\n",
      "      hidden_states[0] = 1\n",
      "\n",
      "      # Recalculate the probabilities that the visible units are on.\n",
      "      visible_activations = np.dot(hidden_states, self.weights.T)\n",
      "      visible_probs = self._logistic(visible_activations)\n",
      "      visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
      "      samples[i,:] = visible_states\n",
      "\n",
      "    # Ignore the bias units (the first column), since they're always set to 1.\n",
      "    return samples[:,1:]        \n",
      "      \n",
      "  def _logistic(self, x):\n",
      "    return 1.0 / (1 + np.exp(-x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "  \n",
      "  r = RBM()\n",
      "    \n",
      "  training_data = np.array([[1,1,1,0,0,0],\n",
      "                            [1,0,1,0,0,0],\n",
      "                            [1,1,1,0,0,0],\n",
      "                            [0,0,1,1,1,0],\n",
      "                            [0,0,1,1,0,0],\n",
      "                            [0,0,1,1,1,0]])\n",
      "  r.train(training_data)\n",
      "#   print('Weights:')\n",
      "#   print(r.weights)\n",
      "  user = np.array([[0,0,0,1,1,0]])\n",
      "  for i in range(5):  \n",
      "    print('For a new user: get a state of the hidden units given visible:')  \n",
      "    print(r.run_visible(user))\n",
      "    print('For a given state of the hidden units: get a daydream:')\n",
      "    print(r.run_hidden(r.run_visible(user)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For a new user: get a state of the hidden units given visible:\n",
        "[[ 0.  0.]]\n",
        "For a given state of the hidden units: get a daydream:\n",
        "[[ 0.  0.  1.  1.  1.  0.]]\n",
        "For a new user: get a state of the hidden units given visible:\n",
        "[[ 0.  0.]]\n",
        "For a given state of the hidden units: get a daydream:\n",
        "[[ 0.  0.  1.  1.  1.  0.]]\n",
        "For a new user: get a state of the hidden units given visible:\n",
        "[[ 0.  0.]]\n",
        "For a given state of the hidden units: get a daydream:\n",
        "[[ 0.  0.  1.  1.  1.  0.]]\n",
        "For a new user: get a state of the hidden units given visible:\n",
        "[[ 0.  0.]]\n",
        "For a given state of the hidden units: get a daydream:\n",
        "[[ 0.  0.  1.  1.  0.  0.]]\n",
        "For a new user: get a state of the hidden units given visible:\n",
        "[[ 0.  0.]]\n",
        "For a given state of the hidden units: get a daydream:\n",
        "[[ 0.  0.  1.  1.  1.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(np.insert)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function insert in module numpy.lib.function_base:\n",
        "\n",
        "insert(arr, obj, values, axis=None)\n",
        "    Insert values along the given axis before the given indices.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    arr : array_like\n",
        "        Input array.\n",
        "    obj : int, slice or sequence of ints\n",
        "        Object that defines the index or indices before which `values` is\n",
        "        inserted.\n",
        "    \n",
        "        .. versionadded:: 1.8.0\n",
        "    \n",
        "        Support for multiple insertions when `obj` is a single scalar or a\n",
        "        sequence with one element (similar to calling insert multiple\n",
        "        times).\n",
        "    values : array_like\n",
        "        Values to insert into `arr`. If the type of `values` is different\n",
        "        from that of `arr`, `values` is converted to the type of `arr`.\n",
        "        `values` should be shaped so that ``arr[...,obj,...] = values``\n",
        "        is legal.\n",
        "    axis : int, optional\n",
        "        Axis along which to insert `values`.  If `axis` is None then `arr`\n",
        "        is flattened first.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    out : ndarray\n",
        "        A copy of `arr` with `values` inserted.  Note that `insert`\n",
        "        does not occur in-place: a new array is returned. If\n",
        "        `axis` is None, `out` is a flattened array.\n",
        "    \n",
        "    See Also\n",
        "    --------\n",
        "    append : Append elements at the end of an array.\n",
        "    concatenate : Join a sequence of arrays together.\n",
        "    delete : Delete elements from an array.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    Note that for higher dimensional inserts `obj=0` behaves very different\n",
        "    from `obj=[0]` just like `arr[:,0,:] = values` is different from\n",
        "    `arr[:,[0],:] = values`.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> a = np.array([[1, 1], [2, 2], [3, 3]])\n",
        "    >>> a\n",
        "    array([[1, 1],\n",
        "           [2, 2],\n",
        "           [3, 3]])\n",
        "    >>> np.insert(a, 1, 5)\n",
        "    array([1, 5, 1, 2, 2, 3, 3])\n",
        "    >>> np.insert(a, 1, 5, axis=1)\n",
        "    array([[1, 5, 1],\n",
        "           [2, 5, 2],\n",
        "           [3, 5, 3]])\n",
        "    \n",
        "    Difference between sequence and scalars:\n",
        "    >>> np.insert(a, [1], [[1],[2],[3]], axis=1)\n",
        "    array([[1, 1, 1],\n",
        "           [2, 2, 2],\n",
        "           [3, 3, 3]])\n",
        "    >>> np.array_equal(np.insert(a, 1, [1, 2, 3], axis=1),\n",
        "    ...                np.insert(a, [1], [[1],[2],[3]], axis=1))\n",
        "    True\n",
        "    \n",
        "    >>> b = a.flatten()\n",
        "    >>> b\n",
        "    array([1, 1, 2, 2, 3, 3])\n",
        "    >>> np.insert(b, [2, 2], [5, 6])\n",
        "    array([1, 1, 5, 6, 2, 2, 3, 3])\n",
        "    \n",
        "    >>> np.insert(b, slice(2, 4), [5, 6])\n",
        "    array([1, 1, 5, 2, 6, 2, 3, 3])\n",
        "    \n",
        "    >>> np.insert(b, [2, 2], [7.13, False]) # type casting\n",
        "    array([1, 1, 7, 0, 2, 2, 3, 3])\n",
        "    \n",
        "    >>> x = np.arange(8).reshape(2, 4)\n",
        "    >>> idx = (1, 3)\n",
        "    >>> np.insert(x, idx, 999, axis=1)\n",
        "    array([[  0, 999,   1,   2, 999,   3],\n",
        "           [  4, 999,   5,   6, 999,   7]])\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}